{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'C:\\\\Users\\\\codyi\\\\Repositories\\\\project-athena\\\\Task3\\\\scripts\\\\cody_scripts', 'C:\\\\Users\\\\codyi\\\\anaconda3\\\\envs\\\\athena\\\\python37.zip', 'C:\\\\Users\\\\codyi\\\\anaconda3\\\\envs\\\\athena\\\\DLLs', 'C:\\\\Users\\\\codyi\\\\anaconda3\\\\envs\\\\athena\\\\lib', 'C:\\\\Users\\\\codyi\\\\anaconda3\\\\envs\\\\athena', 'C:\\\\Users\\\\codyi\\\\anaconda3\\\\envs\\\\athena\\\\lib\\\\site-packages', 'C:\\\\Users\\\\codyi\\\\anaconda3\\\\envs\\\\athena\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\codyi\\\\anaconda3\\\\envs\\\\athena\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\codyi\\\\anaconda3\\\\envs\\\\athena\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\codyi\\\\anaconda3\\\\envs\\\\athena\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\codyi\\\\.ipython', 'C:\\\\Users\\\\codyi\\\\Repositories\\\\project-athena\\\\src']\n"
     ]
    }
   ],
   "source": [
    "## Notebook by Ying Meng, (from notebooks/tasks/Task1_GenerateAEs_ZeroKnowledgeModel)\n",
    "## Modified by Cody Shearer\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "print(sys.path)\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Prepare a smaller dataset for your experiment\n",
    "\n",
    "* python example: `tutorials/subsamples.py`\n",
    "* api: `utils.data.subsampling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import subsampling\n",
    "from utils.file import load_from_json\n",
    "\n",
    "# load the configurations for the experiment\n",
    "data_configs = load_from_json(\"../../configs/BIM/data-bim-mnist.json\")\n",
    "output_root = \"../../data\"\n",
    "\n",
    "# load the full-sized benign samples\n",
    "file = os.path.join(data_configs.get('dir'), data_configs.get('bs_file'))\n",
    "X_bs = np.load(file)\n",
    "\n",
    "# load the corresponding true labels\n",
    "file = os.path.join(data_configs.get('dir'), data_configs.get('label_file'))\n",
    "labels = np.load(file)\n",
    "\n",
    "# get random subsamples\n",
    "# for MNIST, num_classes is 10\n",
    "# files \"subsamples-mnist-ratio_0.1-xxxxxx.npy\" and \"sublabels-mnist-ratio_0.1-xxxxxx.npy\"\n",
    "# will be generated and saved at \"/results\" folder, where \"xxxxxx\" are timestamps.\n",
    "\n",
    "subsamples, sublabels = subsampling(data=X_bs,\n",
    "                                    labels=labels,\n",
    "                                    num_classes=10,\n",
    "                                    filepath=output_root,\n",
    "                                    filename='mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the json file accordingly\n",
    "\n",
    "1. Copy and paste the generated subsamples to the right place (i.e., defined by `\"dir\"` in `data-mnist.json`).\n",
    "2. In the `data-mnist.json`, replace the value of `\"bs_file\"` with the `\"subsamples-mnist-ratio_0.1-xxxxxx.npy\"` and the value of `\"label_file\"` with the `\"sublabels-mnist-ratio_0.1-xxxxxx.npy\"`.\n",
    "\n",
    "# Generate adversarial examples\n",
    "We use `FGSM` as the example.\n",
    "* python example: `tutorials/craft_adversarial_examples.py`\n",
    "* main api: `attacks.attack.generate`\n",
    "* check tunable parameters for each attack in file `attacks/attack.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from tutorials/craft_adversarial_examples.py\n",
    "def generate_ae(model, data, labels, attack_configs, save=False, output_dir=None):\n",
    "    \"\"\"\n",
    "    Generate adversarial examples\n",
    "    :param model: WeakDefense. The targeted model.\n",
    "    :param data: array. The benign samples to generate adversarial for.\n",
    "    :param labels: array or list. The true labels.\n",
    "    :param attack_configs: dictionary. Attacks and corresponding settings.\n",
    "    :param save: boolean. True, if save the adversarial examples.\n",
    "    :param output_dir: str or path. Location to save the adversarial examples.\n",
    "        It cannot be None when save is True.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    img_rows, img_cols = data.shape[1], data.shape[2]\n",
    "    num_attacks = attack_configs.get(\"num_attacks\")\n",
    "    data_loader = (data, labels)\n",
    "\n",
    "    if len(labels.shape) > 1:\n",
    "        labels = np.asarray([np.argmax(p) for p in labels])\n",
    "\n",
    "    # generate attacks one by one\n",
    "    for id in range(num_attacks):\n",
    "        key = \"configs{}\".format(id)\n",
    "        data_adv = generate(model=model,\n",
    "                            data_loader=data_loader,\n",
    "                            attack_args=attack_configs.get(key)\n",
    "                            )\n",
    "        # predict the adversarial examples\n",
    "        predictions = model.predict(data_adv)\n",
    "        predictions = np.asarray([np.argmax(p) for p in predictions])\n",
    "\n",
    "        err = error_rate(y_pred=predictions, y_true=labels)\n",
    "        print(\">>> error rate:\", err)\n",
    "\n",
    "        # plotting some examples\n",
    "        num_plotting = min(data.shape[0], 2)\n",
    "        for i in range(num_plotting):\n",
    "            img = data_adv[i].reshape((img_rows, img_cols))\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            title = '{}: {}->{}'.format(attack_configs.get(key).get(\"description\"),\n",
    "                                        labels[i],\n",
    "                                        predictions[i]\n",
    "                                        )\n",
    "            plt.title(title)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        # save the adversarial example\n",
    "        if save:\n",
    "            if output_dir is None:\n",
    "                raise ValueError(\"Cannot save images to a none path.\")\n",
    "            # save with a random name\n",
    "            file = os.path.join(output_dir,\n",
    "                \"AE-mnist-cnn-clean-bim_eps{}_maxiter{}.npy\".format(\n",
    "                    attack_configs.get(key).get(\"eps\"),\n",
    "                    attack_configs.get(key).get(\"max_iter\"))\n",
    "                )\n",
    "            print(\"Save the adversarial examples to file [{}].\".format(file))\n",
    "            np.save(file, data_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../configs/BIM/model-mnist.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-664e86a98a8a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;31m# loading experiment configurations\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[0mmodel_configs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload_from_json\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"../../configs/BIM/model-mnist.json\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[0mdata_configs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload_from_json\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"../../configs/BIM/data-mnist.json\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[0mattack_configs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload_from_json\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"../../configs/BIM/attack-bim-mnist.json\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\codyi\\Repositories\\project-athena\\src\\utils\\file.py\u001B[0m in \u001B[0;36mload_from_json\u001B[1;34m(file)\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[1;33m:\u001B[0m\u001B[1;32mreturn\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m     \"\"\"\n\u001B[1;32m---> 26\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'r'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mjson_file\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m         \u001B[0mdict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mjson\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mjson_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../../configs/BIM/model-mnist.json'"
     ]
    }
   ],
   "source": [
    "from utils.model import load_lenet\n",
    "from utils.metrics import error_rate\n",
    "from attacks.attack import generate\n",
    "from utils.file import load_from_json\n",
    "\n",
    "# loading experiment configurations\n",
    "model_configs = load_from_json(\"../../configs/BIM/model-mnist.json\")\n",
    "data_configs = load_from_json(\"../../configs/BIM/data-mnist.json\")\n",
    "attack_configs = load_from_json(\"../../configs/BIM/attack-bim-mnist.json\")\n",
    "\n",
    "# load the targeted model\n",
    "model_file = os.path.join(model_configs.get(\"dir\"), model_configs.get(\"um_file\"))\n",
    "target = load_lenet(file=model_file, wrap=True)\n",
    "\n",
    "# load the benign samples\n",
    "\n",
    "data_file = os.path.join(data_configs.get('dir'), data_configs.get('bs_file'))\n",
    "data_bs = np.load(data_file)\n",
    "# load the corresponding true labels\n",
    "label_file = os.path.join(data_configs.get('dir'), data_configs.get('label_file'))\n",
    "labels = np.load(label_file)\n",
    "\n",
    "# generate AEs\n",
    "# in this example, we generate AEs for 5 benign samples\n",
    "# data_bs = data_bs[:5]\n",
    "# labels = labels[:5]\n",
    "# let save=True and specify an output folder to save the generated AEs\n",
    "generate_ae(model=target, data=data_bs, labels=labels, attack_configs=attack_configs, save=True, output_dir=\"../../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the generated adversarial examples in json\n",
    "\n",
    "* Either add an item for the generated AEs. e.g., assume that we named the generated AE as `\"fgsm_eps0.3.npy\"`, then add the item as below example, then get your AE list by `data_configs.get(\"task1_aes\")`.\n",
    "\n",
    "```\n",
    "\"task1_aes\" : [\n",
    "                  \"fgsm_eps0.3.npy\"\n",
    "              ]\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "2. Or, create a new json file similar to `\"data-mnist.json\"`, and replace the whole list for `\"ae_files\"` with your own list.\n",
    "\n",
    "# Evaluate the generated AEs\n",
    "* python example: `tutorials/eval_model.py`\n",
    "* api: `utils.metrics.error_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import load_pool\n",
    "from utils.metrics import error_rate, get_corrections\n",
    "from models.athena import Ensemble, ENSEMBLE_STRATEGY\n",
    "\n",
    "# copied from tutorials/eval_model.py\n",
    "def evaluate(trans_configs, model_configs,\n",
    "             data_configs, save=False, output_dir=None):\n",
    "    \"\"\"\n",
    "    Apply transformation(s) on images.\n",
    "    :param trans_configs: dictionary. The collection of the parameterized transformations to test.\n",
    "        in the form of\n",
    "        { configsx: {\n",
    "            param: value,\n",
    "            }\n",
    "        }\n",
    "        The key of a configuration is 'configs'x, where 'x' is the id of corresponding weak defense.\n",
    "    :param model_configs:  dictionary. Defines model related information.\n",
    "        Such as, location, the undefended model, the file format, etc.\n",
    "    :param data_configs: dictionary. Defines data related information.\n",
    "        Such as, location, the file for the true labels, the file for the benign samples,\n",
    "        the files for the adversarial examples, etc.\n",
    "    :param save: boolean. Save the transformed sample or not.\n",
    "    :param output_dir: path or str. The location to store the transformed samples.\n",
    "        It cannot be None when save is True.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Load the baseline defense (PGD-ADT model)\n",
    "    baseline = load_lenet(file=model_configs.get('pgd_trained'), trans_configs=None,\n",
    "                                  use_logits=False, wrap=False)\n",
    "\n",
    "    # get the undefended model (UM)\n",
    "    file = os.path.join(model_configs.get('dir'), model_configs.get('um_file'))\n",
    "    undefended = load_lenet(file=file,\n",
    "                            trans_configs=trans_configs.get('configs0'),\n",
    "                            wrap=True)\n",
    "    print(\">>> um:\", type(undefended))\n",
    "\n",
    "    # load weak defenses into a pool\n",
    "    pool, _ = load_pool(trans_configs=trans_configs,\n",
    "                        model_configs=model_configs,\n",
    "                        active_list=True,\n",
    "                        wrap=True)\n",
    "    # create an AVEP ensemble from the WD pool\n",
    "    wds = list(pool.values())\n",
    "    print(\">>> wds:\", type(wds), type(wds[0]))\n",
    "    ensemble = Ensemble(classifiers=wds, strategy=ENSEMBLE_STRATEGY.AVEP.value)\n",
    "\n",
    "    # load the benign samples\n",
    "    bs_file = os.path.join(data_configs.get('dir'), data_configs.get('bs_file'))\n",
    "    x_bs = np.load(bs_file)\n",
    "    img_rows, img_cols = x_bs.shape[1], x_bs.shape[2]\n",
    "\n",
    "    # load the corresponding true labels\n",
    "    label_file = os.path.join(data_configs.get('dir'), data_configs.get('label_file'))\n",
    "    labels = np.load(label_file)\n",
    "\n",
    "    # get indices of benign samples that are correctly classified by the targeted model\n",
    "    print(\">>> Evaluating UM on [{}], it may take a while...\".format(bs_file))\n",
    "    pred_bs = undefended.predict(x_bs)\n",
    "    corrections = get_corrections(y_pred=pred_bs, y_true=labels)\n",
    "\n",
    "    # Evaluate AEs.\n",
    "    ae_list = data_configs.get('ae_files')\n",
    "    for ae_ind in range(len(ae_list)):\n",
    "        results = {}\n",
    "        ae_file = os.path.join(data_configs.get('dir'), ae_list[ae_ind])\n",
    "        print(ae_list[ae_ind])\n",
    "        print(ae_file)\n",
    "        x_adv = np.load(ae_file)\n",
    "\n",
    "        # evaluate the undefended model on the AE\n",
    "        print(\">>> Evaluating UM on [{}], it may take a while...\".format(ae_file))\n",
    "        pred_adv_um = undefended.predict(x_adv)\n",
    "        err_um = error_rate(y_pred=pred_adv_um, y_true=labels, correct_on_bs=corrections)\n",
    "        # track the result\n",
    "        results['UM'] = err_um\n",
    "\n",
    "        # evaluate the ensemble on the AE\n",
    "        print(\">>> Evaluating ensemble on [{}], it may take a while...\".format(ae_file))\n",
    "        pred_adv_ens = ensemble.predict(x_adv)\n",
    "        err_ens = error_rate(y_pred=pred_adv_ens, y_true=labels, correct_on_bs=corrections)\n",
    "        # track the result\n",
    "        results['Ensemble'] = err_ens\n",
    "\n",
    "        # evaluate the baseline on the AE\n",
    "        print(\">>> Evaluating baseline model on [{}], it may take a while...\".format(ae_file))\n",
    "        pred_adv_bl = baseline.predict(x_adv)\n",
    "        err_bl = error_rate(y_pred=pred_adv_bl, y_true=labels, correct_on_bs=corrections)\n",
    "        # track the result\n",
    "        results['PGD-ADT'] = err_bl\n",
    "\n",
    "        # TODO: collect and dump the evaluation results to file(s) such that you can analyze them later.\n",
    "        print(\">>> Evaluations on [{}]:\\n{}\".format(ae_file, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading model [../models/baseline/advTrained-mnist-adtC.h5]...\n",
      ">>> Loading model [../models/cnn/model-mnist-cnn-clean.h5]...\n",
      ">>> um: <class 'models.keras.WeakDefense'>\n",
      ">>> Loading model [../models/cnn/model-mnist-cnn-flip_horizontal.h5]...\n",
      ">>> Loading model [../models/cnn/model-mnist-cnn-affine_both_stretch.h5]...\n",
      ">>> Loading model [../models/cnn/model-mnist-cnn-morph_gradient.h5]...\n",
      ">>> Loaded 3 models.\n",
      ">>> wds: <class 'list'> <class 'models.keras.WeakDefense'>\n",
      ">>> Evaluating UM on [../data/subsamples-mnist-ratio_0.1-285633.156.npy], it may take a while...\n",
      "AE-mnist-cnn-clean-bim_eps0.1_maxiter60.npy\n",
      "../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter60.npy\n",
      ">>> Evaluating UM on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter60.npy], it may take a while...\n",
      ">>> Evaluating ensemble on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter60.npy], it may take a while...\n",
      ">>> Evaluating baseline model on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter60.npy], it may take a while...\n",
      ">>> Evaluations on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter60.npy]:\n",
      "{'UM': 0.9264853977844915, 'Ensemble': 0.022155085599194362, 'PGD-ADT': 0.025176233635448138}\n",
      "AE-mnist-cnn-clean-bim_eps0.1_maxiter70.npy\n",
      "../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter70.npy\n",
      ">>> Evaluating UM on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter70.npy], it may take a while...\n",
      ">>> Evaluating ensemble on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter70.npy], it may take a while...\n",
      ">>> Evaluating baseline model on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter70.npy], it may take a while...\n",
      ">>> Evaluations on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter70.npy]:\n",
      "{'UM': 0.9305135951661632, 'Ensemble': 0.022155085599194362, 'PGD-ADT': 0.025176233635448138}\n",
      "AE-mnist-cnn-clean-bim_eps0.1_maxiter80.npy\n",
      "../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter80.npy\n",
      ">>> Evaluating UM on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter80.npy], it may take a while...\n",
      ">>> Evaluating ensemble on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter80.npy], it may take a while...\n",
      ">>> Evaluating baseline model on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter80.npy], it may take a while...\n",
      ">>> Evaluations on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter80.npy]:\n",
      "{'UM': 0.9305135951661632, 'Ensemble': 0.022155085599194362, 'PGD-ADT': 0.025176233635448138}\n",
      "AE-mnist-cnn-clean-bim_eps0.1_maxiter90.npy\n",
      "../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter90.npy\n",
      ">>> Evaluating UM on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter90.npy], it may take a while...\n",
      ">>> Evaluating ensemble on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter90.npy], it may take a while...\n",
      ">>> Evaluating baseline model on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter90.npy], it may take a while...\n",
      ">>> Evaluations on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter90.npy]:\n",
      "{'UM': 0.9305135951661632, 'Ensemble': 0.022155085599194362, 'PGD-ADT': 0.025176233635448138}\n",
      "AE-mnist-cnn-clean-bim_eps0.1_maxiter100.npy\n",
      "../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter100.npy\n",
      ">>> Evaluating UM on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter100.npy], it may take a while...\n",
      ">>> Evaluating ensemble on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter100.npy], it may take a while...\n",
      ">>> Evaluating baseline model on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter100.npy], it may take a while...\n",
      ">>> Evaluations on [../data/AE-mnist-cnn-clean-bim_eps0.1_maxiter100.npy]:\n",
      "{'UM': 0.9335347432024169, 'Ensemble': 0.022155085599194362, 'PGD-ADT': 0.025176233635448138}\n"
     ]
    }
   ],
   "source": [
    "# load experiment configurations\n",
    "trans_configs = load_from_json(\"../src/configs/task1/athena-mnist.json\")\n",
    "model_configs = load_from_json(\"../src/configs/task1/model-mnist.json\")\n",
    "data_configs = load_from_json(\"../src/configs/task1/data-bim-mnist.json\")\n",
    "\n",
    "output_dir = \"../results\"\n",
    "\n",
    "# evaluate\n",
    "evaluate(trans_configs=trans_configs,\n",
    "         model_configs=model_configs,\n",
    "         data_configs=data_configs,\n",
    "         save=False,\n",
    "         output_dir=output_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}