{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Team Ares -- Task 1 Report -- Fall 2020\n",
    "## Contributions:\n",
    "### Cody Shearer\n",
    "- Code:\n",
    "  - Generated BIM AEs\n",
    "  - Evalauated BIM AEs\n",
    "- Report:\n",
    "  - Background\n",
    "  - BIM Attack and Evaluation\n",
    "- Created/managed team repository.\n",
    "- Helped setup development environments.\n",
    "- Organized team meetings.\n",
    "\n",
    "### Zhymir Thompson\n",
    "- Performed experiments, gathered results for Carlini Wagner attack.\n",
    "\n",
    "### Mahmudul Hasan\n",
    "- Performed JSMA experiment, did evaluation and wrote JSMA report.\n",
    "\n",
    "### Vincent Davidson\n",
    "- Co-managed team meetings\n",
    "- Co-managed/organized individual contributions for each team member\n",
    "- Performed experiments, evaluation and analysis on PGD attacks. \n",
    "\n",
    "__\n",
    "## Background\n",
    "In their work on ATHENA, Ying et al. (2020) solve the problem of adversarial defense, not as a technique, but as a framework, wherein a variable number of weak adversarial defenses (an ensemble) are trained and their collective predictions are used to create a response to adversarial attacks, the robustness and overhead of which are inversely correlated and controlled by the number of weak defenses.\n",
    "\n",
    "In the following report, we compare the robustness of ATHENA's ensemble with PGD-ADT and an undefended (control) model by subjecting them to several varations of different adversarial attack methods. \n",
    "\n",
    "## BIM Attack and Evaluation\n",
    "### Introduction\n",
    "The basic iterative method (BIM) is a whitebox adversarial attack developed by researchers at Google Brain and OpenAI. In their paper, Kurakin et. al demonstrate transferability of adversarial examples from a lab setting to a real-world setting. In particular, they show that adversarial examples generated by attackers who have direct access to an image classifier can still fool that same model when the images are seen through a physical camera. Furthermore, they found that no modification of the attack was needed to account for the camera.\n",
    "\n",
    "### Experimental Setting\n",
    "Here we consider an adversarial attack on a convolutional neural network (CNN) trained on a subset (10%) of the MNIST dataset using ten variations of the [basic iterative method](https://arxiv.org/pdf/1607.02533.pdf) (BIM). We first hold the epsilon value constant at 0.10 while varying the maximum number of iterations, then we hold maximum number of iterations at 70 and vary the epsilon to reveal how these parameters influence the error rate of the undefeneded model (UM), an athena ensemble, and PGD-ADT. \n",
    "\n",
    "First experiment:\n",
    "  - epsilon: 0.10\n",
    "  - max_iter: 100, 90, 80, 70, 60\n",
    "\n",
    "Second experiment:\n",
    "- epsilon: 0.20, 0.30, 0.40, 0.50, 0.60\n",
    "- max_iter: 70\n",
    "\n",
    "Using the following configurations, we generate AEs and evaluate their effectivness against the UM, the ensemble model, and PGD-ADT, using `notebooks/Task1_GenerateAEs_ZeroKnowledgeModel.ipynb` for the first\n",
    "- `src/configs/task1/athena-mnist.json`\n",
    "- `src/configs/task1/attack-bim-mnist.json`\n",
    "- `src/configs/task1/data-bim-mnist.json`\n",
    "\n",
    "The AEs can be found at: \n",
    "- `AE-mnist-cnn-clean-bim_eps0.1_maxiter60.npy`\n",
    "- `AE-mnist-cnn-clean-bim_eps0.1_maxiter70.npy`\n",
    "- `AE-mnist-cnn-clean-bim_eps0.1_maxiter80.npy`\n",
    "- `AE-mnist-cnn-clean-bim_eps0.1_maxiter90.npy`\n",
    "- `AE-mnist-cnn-clean-bim_eps0.1_maxiter100.npy`\n",
    "\n",
    "### Undefended Model Results\n",
    "We find that the error rate drops only for the UM and only twice. We would expect these drops to occur only at 70 and 60, perhaps as some upper bound is reached. However, we find the interesting result that the drop in error rate occurs only from 100 to 90 and 70 to 60; the error rate is the same for 90, 80, and 70. \n",
    "\n",
    "### Ensemble and PGD-ADT Results\n",
    "The ensemble has nearly the same error rate as PGD-ADT, which in all cases is about 2%. \n",
    "\n",
    "| BIM Error Rate (epsilon=0.1) |             |             |             |                                             |                                             |\n",
    "|------------------------|-------------|-------------|-------------|---------------------------------------------|---------------------------------------------|\n",
    "| Max Iterations         | UM          | Ensemble    | PGD-ADT     | 9->1                                        | 4->9                                        |\n",
    "| 100                    | 0.933534743 | 0.022155086 | 0.025176234 | ![](figures/bim_eps0.1_maxiter90_9to1.png)  | ![](figures/bim_eps0.1_maxiter100_4to9.png) |\n",
    "| 90                     | 0.930513595 | 0.022155086 | 0.025176234 | ![](figures/bim_eps0.1_maxiter90_9to1.png)  | ![](figures/bim_eps0.1_maxiter90_4to9.png)  |\n",
    "| 80                     | 0.930513595 | 0.022155086 | 0.025176234 | ![](figures/bim_eps0.1_maxiter80_9to1.png)  | ![](figures/bim_eps0.1_maxiter80_4to9.png)  |\n",
    "| 70                     | 0.930513595 | 0.022155086 | 0.025176234 | ![](figures/bim_eps0.1_maxiter70_9to1.png)  | ![](figures/bim_eps0.1_maxiter70_4to9.png)  |\n",
    "| 60                     | 0.926485398 | 0.022155086 | 0.025176234 | ![](figures/bim_eps0.1_maxiter60_9to1.png)  | ![](figures/bim_eps0.1_maxiter60_4to9.png)  |\n",
    "\n",
    "In conclusion, BIM is only effective against the UM, with the erorr rates of the ensemble model and PGD-ADT being around 2%. Changes to the maximum iterations for BIM only have a (slight) effect on the UM, with the ensemble model and PGD-ADT defenses seeing no change.\n",
    "___\n",
    "\n",
    "\n",
    "## CW Attack and Evaluation\n",
    "\n",
    "For the CW attack, the variables altered were the learning rate and normalization method. There are a total of 10 variations where 5 learning rates are repeated for each normalization method. The files for this attack are stored in ~/src/task1/attack2.\n",
    "\n",
    "### Files Used\n",
    "\n",
    "Configs:\n",
    "* athena-mnist.json\n",
    "* attack-config.json\n",
    "* data-config.json\n",
    "* model-config.json\n",
    "* ./results/sub-data-config.json\n",
    "\n",
    "sub-samples:\n",
    "\n",
    "* sublabels-10-ratio_0.001-477207.062.npy\n",
    "* subsamples-10-ratio_0.001-477207.062.npy\n",
    "\n",
    "AE's:\n",
    "\n",
    "* cw-L2-lr0.2.npy\n",
    "* cw-L2-lr0.02.npy\n",
    "* cw-L2-lr0.002.npy\n",
    "* cw-L2-lr0.00002.npy\n",
    "* cw-L2-lr0.99.npy\n",
    "* cw-Lf-lr0.2.npy\n",
    "* cw-Lf-lr0.02.npy\n",
    "* cw-Lf-lr0.002.npy\n",
    "* cw-Lf-lr0.00002.npy\n",
    "* cw-Lf-lr0.99.npy\n",
    "\n",
    "_\\*AEs located in ./results folder_\n",
    "\n",
    "### Results\n",
    "\n",
    "#### Summary and Analysis\n",
    "\n",
    "The L2 vs LINF norm showed a higher error rate result overall with LINF normalization for most cases.\n",
    "The data also supports the idea that high learning rates have a slight edge over weak learning rates, but the actual difference is insignificant and could easily disappear given a larger sample size.\n",
    "The data was only trained on a sample size of 10 due to the large increase time for a factor increase of 10.\n",
    "\n",
    "#### Data\n",
    "\n",
    "|Norm |LR  |UM      |Ensemble|PGD-ADT|  |\n",
    "|-----|----|--------|------|-----|-----|\n",
    "|L2   |0.02|0.4     |0.0   |0.0  |     |\n",
    "|L2   |0.2 |0.5     |0.0   |0.1  |     |\n",
    "|L2|0.002|0.5|0.0|0.0|\n",
    "|L2|0.99|0.6|0.0|0.1|\n",
    "|L2|0.00002|0.8|0.8|0.8|\n",
    "|LINF|0.02|0.8|0.8|0.8|\n",
    "|LINF|0.2|0.8|0.8|0.8|\n",
    "|LINF|0.002|0.8|0.8|0.8|\n",
    "|LINF|0.99|0.8|0.8|0.8|\n",
    "|LINF|0.00002|0.7|0.8|0.8|\n",
    "\n",
    "\n",
    "## JSMA Attack and Evaluation\n",
    "We worked with JSMA attack on a convolutional neural network (CNN). We used the following values for gamma \n",
    "\n",
    "gamma: 0.30, 0.40, 0.50, 0.60, 0.70\n",
    "\n",
    "By using notebooks/Task1_GenerateAEs_ZeroKnowledgeModel.ipynb:\n",
    "* src/practice task1/at.json\n",
    "* src/practice task1/md.json\n",
    "* src/practice task1/dt.json\n",
    "We can get  AEs at:\n",
    "* 215272.937.npy\n",
    "* 215373.062.npy\n",
    "* 215373.156.npy\n",
    "* 215373.25.npy\n",
    "* 215373.343.npy\n",
    "\n",
    "### Undefended Model Results\n",
    "We can see  that the error rate was reduced  for the UM and it reduced five times, where Y predicted shape is 10. Moreover, Ensemble and JSMA are also zero. Furthermore, at 215272.937.npy, 215373.062.npy, 215373.156.npy, \n",
    "215373.25.npy, 215373.343.npy, the value of UM, Ensemble and JSMA are zero.\n",
    "\n",
    "\n",
    "\n",
    "## PGD Attack and Evaluation \n",
    "- The PGD attack is trained on a subset of the MNIST dataset using five variations of the attack. The variables altered were the epsilon between the number 0 and 1. \n",
    "\n",
    "## Files Used\n",
    "Configs: \t\n",
    "* athena-mnist.json\n",
    "* attack-zk-mnist.json\n",
    "* data-mnist.json\n",
    "* model-mnist.json\n",
    "* ./results/sample.json\n",
    "\n",
    "Sub-samples:\n",
    "* sublabels-10-ratio_0.001-5.142948666.npy\n",
    "* subsamples-10-ratio_0.001-5.142948666.npy\n",
    "\n",
    "AEâ€™s:\n",
    "* pgd-eps0.1.npy\n",
    "* pgd-eps0.8.npy\n",
    "* pgd-eps0.7.npy\n",
    "* pgd-eps0.5.npy\n",
    "* pgd-eps0.3npy\n",
    "\n",
    "*AEs located in ./results folder\n",
    "\n",
    "## Summary and Analysis \n",
    "\n",
    "The epsilon showed a consistent error rate of each variation between the range of 0.7 and 1.0, indicating the number of inputs that fools the model. So, the higher the number closest to 1.0 the better.  \n",
    "\n",
    "## Data:\n",
    "\n",
    "### Ensemble and PGD-ADT Results\n",
    "\n",
    "| PGD Error Rate |           |            |            |                  |\n",
    "|------------------------|-------------|-------------|---------------------------------------------|---------------------------------------|\n",
    "|                    |   UM     |   Ensemble   |   PGD-ADT  |\n",
    "|    Epsilon=0.3    |    0.8   |     0.9      |    1.0     |\n",
    "|    Epsilon=0.5    |    0.7   |     0.9      |    0.8     |\n",
    "|    Epsilon=0.1    |    0.9   |     1.0      |    1.0     |\n",
    "|    Epsilon=0.7    |    0.7   |     0.9      |    0.7     |\n",
    "|    Epsilon=0.8    |    0.7   |     0.9      |    0.7     |    \n",
    "\n",
    "## Citations\n",
    "- [Dong, Yinpeng, et al. (2020)](https://arxiv.org/pdf/2002.05999.pdf) \"Adversarial Distributional Training for Robust Deep Learning.\" Advances in Neural Information Processing Systems 33.\n",
    "- [Kurakin, Alexey, Ian Goodfellow, and Samy Bengio. (2016) ](https://arxiv.org/pdf/1607.02533.pdf) \"Adversarial examples in the physical world.\" arXiv preprint arXiv:1607.02533.\n",
    "- [Meng, Ying, et al. (2020)](https://arxiv.org/abs/2001.00308) \"Ensembles of many diverse weak defenses can be strong: defending deep neural networks against adversarial attacks.\" arXiv preprint arXiv:2001.00308.\n",
    "- [LeCun, Y. & Cortes, C. (2010)](http://yann.lecun.com/exdb/mnist/), 'MNIST handwritten digit database', . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
